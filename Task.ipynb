{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'simulation_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = ['clear-night', 'clear-noon', 'clear-sunset', 'haze-noon', 'haze-sunset', 'rain-noon']\n",
    "csvs = ['ctl', 'cvip', 'traj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "dfs = {}\n",
    "for folder in os.listdir(data_dir):\n",
    "    folder_path = os.path.join(data_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for scenario in scenarios:\n",
    "            if scenario in folder:\n",
    "                folder = scenario\n",
    "                break\n",
    "\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                for csv_type in csvs:\n",
    "                    if csv_type in file:\n",
    "                        file = csv_type\n",
    "                        break\n",
    "                all_data.append((folder, file, df))\n",
    "                key = f\"{folder}_{file}\"\n",
    "                dfs[key] = df\n",
    "\n",
    "for i, (folder, file, df) in enumerate(all_data):\n",
    "    print(f\"Data from folder '{folder}', file '{file}':\\n\", df.head(), \"\\n\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the first 5 rows of the ctl.csv, cvip.csv, and traj.csv with the clear-night weather condition.\n",
    "scenario = 'clear-night'\n",
    "\n",
    "for file_type in csvs:\n",
    "    key = f\"{scenario}_{file_type}\"\n",
    "    df = dfs.get(key)\n",
    "    if df is not None:\n",
    "        # print(df['agent_id'].nunique())\n",
    "        print(f\"Data from '{key}':\\n\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. The duration of the scene\n",
    "duration = 0\n",
    "for scenario in scenarios:\n",
    "    for file_type in csvs:\n",
    "        key = f\"{scenario}_{file_type}\"\n",
    "        df = dfs.get(key)\n",
    "        if df is not None:\n",
    "            start_time = df['ts'].min()\n",
    "            end_time = df['ts'].max()\n",
    "            duration = end_time - start_time\n",
    "\n",
    "    print(f\"Scene '{scenario}'\")\n",
    "    print(f\"The duration of the scene: {duration} fps\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Mean and standard deviation of the values of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['throttle', 'steer', 'brake', 'cvip', 'x', 'y', 'v']\n",
    "results_df = pd.DataFrame(index=features)\n",
    "\n",
    "for scenario in scenarios:\n",
    "    mean_std_values = {}\n",
    "    for file_type in csvs:\n",
    "        key = f\"{scenario}_{file_type}\" \n",
    "        df = dfs.get(key) \n",
    "        if df is not None:\n",
    "            for feature in features:\n",
    "                if feature in df.columns:\n",
    "                    mean_value = df[feature].mean()\n",
    "                    std_value = df[feature].std()\n",
    "                    mean_std_values[feature] = f\"{mean_value:.3f} ({std_value:.3f})\"\n",
    "\n",
    "    if mean_std_values:\n",
    "        results_df[scenario] = pd.Series(mean_std_values)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel('df.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors, matching with scenarios\n",
    "colors = ['tomato', 'crimson', 'darkorange', 'cornflowerblue', 'lightseagreen', 'lightslategrey']\n",
    "\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f\"{feature.capitalize()} vs Frames for different weather conditions\")\n",
    "    plt.xlabel(\"Frames (ts)\")  \n",
    "    plt.ylabel(f\"{feature.capitalize()} value\")\n",
    "\n",
    "    for i, (scenario, color) in enumerate(zip(scenarios, colors)):\n",
    "        df = None\n",
    "        for file_type in csvs:\n",
    "            key = f\"{scenario}_{file_type}\"\n",
    "            df = dfs.get(key)\n",
    "            if df is not None and feature in df.columns:\n",
    "                df['ts'] = df['ts'].astype(float) \n",
    "                plt.plot(df['ts'], df[feature], label=scenario, color=color, alpha=0.7)\n",
    "\n",
    "    plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True)) \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Weather Conditions\", loc='lower right')\n",
    "    plt.tight_layout()  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on your intuition and life experience, which of the features do you think will change during an accident? How will the feature(s) change? \n",
    "1. `Throttle`: The throttle may decrease sharply just before the accident, as drivers usually slow down or stop when sensing a collision is imminent.\n",
    "2. `Brake`: The brake input may increase dramatically during an accident, indicating the driverâ€™s attempt to stop the vehicle.\n",
    "3. `Steering`: The steering may show rapid changes during an accident, as the driver might try to swerve to avoid an object or another vehicle.\n",
    "4. `Speed(v)`: Speed will typically decrease and may even reach zero as the vehicle comes to a stop due to the accident.\n",
    "5. `Distance to other objects(cvip)`: The distance between the vehicle and other vehicles or objects will decrease rapidly as the accident approaches.\n",
    "6. `x(horizontal direction)`: If the driver or the autonomous system attempts to swerve before the accident, x may undergo rapid changes.\n",
    "7. `y(driving direction)`: If the vehicle decelerates or stops before the accident, y will gradually decrease and may eventually approach zero.\n",
    "\n",
    "##### By looking at the plots you generated in Task 1.3, which weather condition(s) has an accident? \n",
    "Based on the observations: \n",
    "1. The brake value(brake=1) remains at 1, indicating the vehicle applied full braking. \n",
    "2. The speed(v) drops and does not increase again, suggesting the vehicle may have come to a stop.\n",
    "3. The cvip value equals 0, meaning the distance between the vehicle and another object or vehicle is zero, implying a collision.\n",
    "\n",
    "From these points, it can be inferred that an accident occurred under the **`rain-noon`** weather condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task1.3 plot time as an ordinal variable(additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0  \n",
    "\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(10, 6)) \n",
    "    plt.title(f\"{feature.capitalize()} vs Time for different weather conditions\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(f\"{feature.capitalize()} value\")\n",
    "\n",
    "    for i, (scenario, color) in enumerate(zip(scenarios, colors)):\n",
    "        df = None \n",
    "        for file_type in csvs:\n",
    "            key = f\"{scenario}_{file_type}\"  \n",
    "            df = dfs.get(key) \n",
    "            if df is not None and feature in df.columns:\n",
    "                plt.plot(df.index + offset, df[feature], label=scenario, color=color, alpha=0.7)\n",
    "                offset += len(df)\n",
    "\n",
    "    plt.xticks(rotation=45)  \n",
    "    # plt.legend(title=\"Weather Conditions\", loc='lower right')\n",
    "    plt.tight_layout()  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task1.3 subplot(additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3  \n",
    "nrows = (len(scenarios) + ncols - 1) // ncols  \n",
    "\n",
    "for feature in features:\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(12, 6 * nrows))  \n",
    "\n",
    "    ax = ax.flatten()  \n",
    "    for i, (scenario, color) in enumerate(zip(scenarios, colors)):\n",
    "        df = None  \n",
    "        for file_type in csvs:\n",
    "            key = f\"{scenario}_{file_type}\"  \n",
    "            df = dfs.get(key)  \n",
    "            if df is not None and feature in df.columns:\n",
    "                ax[i].plot(df['ts'], df[feature], label=scenario, color=color, alpha=0.7)\n",
    "                ax[i].set_title(f\"{feature.capitalize()} vs Frames for {scenario}\")\n",
    "                ax[i].set_xlabel(\"Frames (ts)\")  \n",
    "                ax[i].set_ylabel(f\"{feature.capitalize()} value\")\n",
    "                # ax[i].legend(loc='lower right')\n",
    "\n",
    "    for j in range(i + 1, len(ax)):\n",
    "        fig.delaxes(ax[j])\n",
    "\n",
    "    plt.tight_layout()  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Turn txt files into a dataframe first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the route_highway.txt file\n",
    "def read_route_highway_txt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        # Extract necessary fields from JSON-like data\n",
    "        return {\n",
    "            'duration_game': data['_checkpoint']['records'][0]['meta']['duration_game'],\n",
    "            'duration_system': data['_checkpoint']['records'][0]['meta']['duration_system'],\n",
    "            'route_length': data['_checkpoint']['records'][0]['meta']['route_length'],\n",
    "            'score_composed': data['_checkpoint']['records'][0]['scores']['score_composed'],\n",
    "            'score_penalty': data['_checkpoint']['records'][0]['scores']['score_penalty'],\n",
    "            'score_route': data['_checkpoint']['records'][0]['scores']['score_route'],\n",
    "            'status': data['_checkpoint']['records'][0]['status']\n",
    "        }\n",
    "\n",
    "# Function to read CSV files\n",
    "def read_csv_file(file_path):\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_csv_df = []\n",
    "df_list = []  # Initialize the df_list to store all the DataFrames\n",
    "\n",
    "for scenario in scenarios:\n",
    "    folder_path = os.path.join(data_dir, f'route_highway_epoch24_{scenario}_fi_ghost_cutin')\n",
    "    files_in_folder = os.listdir(folder_path)\n",
    "    print(f\"Files in folder {folder_path}: {files_in_folder}\")\n",
    "\n",
    "    # Read the route_highway.txt file\n",
    "    route_highway_txt_path = os.path.join(folder_path, 'route_highway.txt')\n",
    "\n",
    "    if os.path.exists(route_highway_txt_path):\n",
    "        route_highway_data = read_route_highway_txt(route_highway_txt_path)\n",
    "        route_highway_df = pd.DataFrame([route_highway_data])\n",
    "    else:\n",
    "        print(f\"route_highway.txt not found in {folder_path}\")\n",
    "        continue  # Skip to the next scenario if the file is not found\n",
    "\n",
    "    # Concatenate the route_highway_df with merged_csv_df\n",
    "    full_df = pd.concat([pd.DataFrame(merged_csv_df), pd.concat([route_highway_df], ignore_index=True)], axis=1)\n",
    "    full_df['scenario'] = scenario\n",
    "    df_list.append(full_df)\n",
    "\n",
    "# After the loop, you can concatenate all dataframes in df_list if needed\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Combine all scenario data into one dataframe\n",
    "if df_list:\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "else:\n",
    "    print(\"No valid dataframes were created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose each simulation run has a result of accident/non-accident, calculate the probability of accident (counts, marginal probability).\n",
    "# Count the number of accidents ('Failed') and total runs\n",
    "accident_count = final_df[final_df['status'] == 'Failed'].shape[0]\n",
    "total_runs = final_df.shape[0]\n",
    "\n",
    "# Calculate the probability of an accident\n",
    "probability_of_accident = accident_count / total_runs\n",
    "\n",
    "# Display the results\n",
    "print(f\"Total runs: {total_runs}\")\n",
    "print(f\"Accidents: {accident_count}\")\n",
    "print(f\"Probability of accident: {probability_of_accident:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each scenario\n",
    "df_list = []\n",
    "for scenario in scenarios:\n",
    "    folder_path = os.path.join(data_dir, f'route_highway_epoch24_{scenario}_fi_ghost_cutin')\n",
    "    files_in_folder = os.listdir(folder_path)\n",
    "    print(f\"Files in folder {folder_path}: {files_in_folder}\")\n",
    "\n",
    "    # Read the route_highway.txt file\n",
    "    route_highway_txt_path = os.path.join(folder_path, 'route_highway.txt')\n",
    "\n",
    "    if os.path.exists(route_highway_txt_path):\n",
    "        route_highway_data = read_route_highway_txt(route_highway_txt_path)\n",
    "        route_highway_df = pd.DataFrame([route_highway_data])\n",
    "    else:\n",
    "        print(f\"route_highway.txt not found in {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    # Initialize an empty dictionary to hold CSV data for each scenario\n",
    "    csv_data_dict = {}\n",
    "    # Read the CSV files (ctl, cvip, traj)\n",
    "    for csv_type in csvs:\n",
    "        matching_files = [f for f in files_in_folder if f.endswith(f'single_{csv_type}.csv')]\n",
    "        if matching_files:\n",
    "            csv_file_path = os.path.join(folder_path, matching_files[0])\n",
    "            csv_df = read_csv_file(csv_file_path)\n",
    "            csv_data_dict[csv_type] = csv_df\n",
    "        else:\n",
    "            print(f\"No file ending with 'single_{csv_type}.csv' found in {folder_path}\")\n",
    "\n",
    "    # Merge CSV dataframes on 'ts','agent_id' column\n",
    "    if 'ctl' in csv_data_dict:\n",
    "        merged_csv_df = csv_data_dict['ctl']\n",
    "        for csv_type in ['cvip', 'traj']:\n",
    "            if csv_type in csv_data_dict:\n",
    "                merged_csv_df = pd.merge(merged_csv_df, csv_data_dict[csv_type], on=['ts','agent_id'], how='left')\n",
    "\n",
    "        # Concatenate the route_highway data to match the length of merged CSV dataframe\n",
    "        data_df = pd.concat([merged_csv_df, pd.concat([route_highway_df]*len(merged_csv_df), ignore_index=True)], axis=1)\n",
    "        data_df['scenario'] = scenario\n",
    "        df_list.append(data_df)\n",
    "\n",
    "# Combine all scenario data into one dataframe\n",
    "if df_list:\n",
    "    data_df = pd.concat(df_list, ignore_index=True)\n",
    "else:\n",
    "    print(\"No valid dataframes were created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. We study the following features: â€œbrakeâ€, â€œsteerâ€, â€œvâ€, â€œyâ€, â€œxâ€, â€œcvipâ€, â€œthrottleâ€. Plot the distribution of each feature for the abnormal runs (including the accident runs) vs normal runs. Treat the values at each time point as an independent individual sample and generate the density plot of the distribution. Describe the difference between the â€œsteerâ€ distribution for normal and abnormal runs.\n",
    "# Classify runs as normal or abnormal based on the 'status' field\n",
    "data_df['run_type'] = data_df['status'].apply(lambda x: 'Normal' if x == 'Completed' else 'Abnormal')\n",
    "\n",
    "# List of features to analyze\n",
    "features = ['brake', 'steer', 'v', 'y', 'x', 'cvip', 'throttle']\n",
    "\n",
    "# Plot distribution for each feature\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Generate density plot for normal and abnormal runs\n",
    "    sns.kdeplot(data=data_df[data_df['run_type'] == 'Normal'], x=feature, label='Normal', fill=True, alpha=0.5)\n",
    "    sns.kdeplot(data=data_df[data_df['run_type'] == 'Abnormal'], x=feature, label='Abnormal', fill=True, alpha=0.5)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(f'Distribution of {feature} for Normal vs Abnormal Runs', fontsize=14)\n",
    "    plt.xlabel(f'{feature}', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. \n",
    "Null Hypothesis (Hâ‚€): The mean of the *`steer`* values for abnormal runs is equal to the mean of the *`steer`* values for normal runs.\\\n",
    "ð»0: ðœ‡ abnormal = ðœ‡ normal\n",
    "\n",
    "Alternative Hypothesis (Hâ‚): The mean of the *`steer`* values for abnormal runs is different from the mean of the *`steer`* values for normal runs.\\\n",
    "ð»1: ðœ‡ abnormal â‰  ðœ‡ normal\n",
    "##### ii. \n",
    "We use Levene's test to check whether the variances between two groups of data (normal and abnormal steer values) are equal. Checking the equality of variances is important before performing a t-test, as it determines which version of the t-test should be used. It is a robust test for equality of variances and works well even when the data is not normally distributed.\n",
    "* `Levene's test result`: Based on the result (Statistic: 38.9101, p-value: 0.0000), we chose Welch's t-test for further analysis.\n",
    "* `T-Test (Welch's T-test) result`: T-test statistic: 3.9279, p-value: 0.0001\n",
    "##### iii. \n",
    "Based on the results of both the Leveneâ€™s test and Welchâ€™s t-test, we conclude that the steering behavior (steer values) during abnormal runs is significantly different from that during normal runs. This difference is statistically significant at the 0.05 significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b.\n",
    "# Classify runs as normal or abnormal based on the 'status' field\n",
    "data_df['run_type'] = data_df['status'].apply(lambda x: 'Normal' if x == 'Completed' else 'Abnormal')\n",
    "\n",
    "# Separate steer values for normal and abnormal runs\n",
    "normal_steer = data_df[data_df['run_type'] == 'Normal']['steer']\n",
    "abnormal_steer = data_df[data_df['run_type'] == 'Abnormal']['steer']\n",
    "\n",
    "# Remove NaN or infinite values from both normal and abnormal steer values\n",
    "normal_steer = normal_steer.dropna()\n",
    "abnormal_steer = abnormal_steer.dropna()\n",
    "\n",
    "# Check for infinite values (inf)\n",
    "normal_steer = normal_steer[~normal_steer.isin([float('inf'), float('-inf')])]\n",
    "abnormal_steer = abnormal_steer[~abnormal_steer.isin([float('inf'), float('-inf')])]\n",
    "\n",
    "# Step 1: Test for equal variances using Leveneâ€™s test\n",
    "stat, p_value_var = stats.levene(normal_steer, abnormal_steer)\n",
    "\n",
    "print(f'Leveneâ€™s test statistic: {stat:.4f}, p-value: {p_value_var:.4f}')\n",
    "\n",
    "# Step 2: Perform the appropriate t-test based on the result of Leveneâ€™s test\n",
    "if p_value_var > 0.05:\n",
    "    # Variances are equal, use the standard 2-sample t-test (pooled variance)\n",
    "    t_stat, p_value_t = stats.ttest_ind(normal_steer, abnormal_steer, equal_var=True)\n",
    "else:\n",
    "    # Variances are not equal, use Welch's t-test (unequal variances)\n",
    "    t_stat, p_value_t = stats.ttest_ind(normal_steer, abnormal_steer, equal_var=False)\n",
    "\n",
    "print(f'T-test statistic: {t_stat:.4f}, p-value: {p_value_t:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b.\n",
    "selected = data_df[['steer', 'cvip', 'v']]\n",
    "correlation_matrix = selected.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Perform the KS two-sample test and calculate its statistics.\n",
    "features = ['steer', 'cvip', 'v']\n",
    "\n",
    "for feature in features:\n",
    "    normal_values = data_df[data_df['status'] == 'Completed'][feature]\n",
    "    abnormal_values = data_df[data_df['status'] == 'Failed'][feature]\n",
    "\n",
    "    ks_statistic, p_value = stats.ks_2samp(normal_values, abnormal_values)\n",
    "\n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(f\"KS Statistic: {ks_statistic}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "    print(\"-\" * 30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Repeat the same test on a feature that you did not select as an indicator of abnormal behavior in Task 2.5. What is your conclusion?\n",
    "features_nselected = ['brake', 'y', 'x', 'throttle']\n",
    "\n",
    "for feature in features_nselected:\n",
    "    normal_values = data_df[data_df['status'] == 'Completed'][feature]\n",
    "    abnormal_values = data_df[data_df['status'] == 'Failed'][feature]\n",
    "\n",
    "    ks_statistic, p_value = stats.ks_2samp(normal_values, abnormal_values)\n",
    "\n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(f\"KS Statistic: {ks_statistic}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "    print(\"-\" * 30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task2.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
